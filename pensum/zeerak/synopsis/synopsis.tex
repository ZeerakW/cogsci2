\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
% \usepackage{minted}
\usepackage{listingsutf8}
\usepackage{url}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{adjustbox} % Used to scale size of tables
\usepackage{acl2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{array}

\begin{document}
\sloppy
\title{Cognitive Science 2 synopsis}
\author{Zeerak Waseem Butt (csp265)}
\date{\today}
\maketitle
\section{Abstract}
Given the increased use of digital media to communicate over time, it is necessary to create systems that can more accurately communicate as intended. In this project I will implement a machine learning system, that is trained on human hands gesturing 'thumbs up' and 'peace'. The system is tested on drawn images of the same gestures, thus indicating that further research into machine learning systems on drawn gestures can be used to preprogram human gestures in human computer interaction as well as computer mediated human interaction.\\

Note: The code is implemented with Klaes Rasmussen, as we both wanted to do image recognition on gestures and it allows us to test on each others training set. The suggested improvements, if there is further research, is more likely to be robust if both systems are worked upon.\\
\section{Introduction}
I am seeking to investigate cross domain adaptation of features in images portraying emblems onto images portraying the same drawn or animated emblems. Given the increase digital communication, as documented in \newcite{pew} and the different forms of interacting with a device, from voice to image recognition it is necessary to be able to map gestures, icons, and words to and from digital media, ensuring that our message is delivered with as little loss of non-verbal information as possible.\\
An act of face to face communication will often include gestures to indicate emotions, as \newcite{kipp} argue. Upon using digital media this mode of non-verbal communication is lost as well as the intonation of the speaker. In order to bridge this gap there digital communications, emoticons and smileys have been taken into use, as \newcite{yuasa} document. \newcite{wachs} divulge that integrating systems for multimodal communication using next generation IT systems will require not only effective and fast mapping from physical space and movement, but also the accurate translation between digital representations of gestures and physcally performed gestures. This is particularly important, considering the results achieved by \newcite{yuasa} showing that abstract representation of faces communicate emotion, thus it is prudent to consider whether representations of gestures elicit a similar reaction.\\
Due to the time constraings I will focus on mapping emblems performed by humans in still images onto digital representations of the same emblems in this study.\\
\section{Previous Work}
\newcite{ekman} argues that not only is there a set of emotions but also that in encounters between "organisms" conveying emotions have a critical role in prompting the feeling organism to react quickly to a given impression. The research \newcite{ekman} performed can easily both directly and indirectly be related to multimodal communication, one such indirect manner is \newcite{ruiter}, who argues that there exist three different models of how communication happens and particularly the language model corresponds with our assumption on how language and gesture interact due to which we base our claims on it.\\

The research \newcite{ekman} and \newcite{ruiter} amongst others did allows further work such as the study by \newcite{kipp} investigating the link between gestures and emotions. Asides from \newcite{kipp} a lot of effort has gone into the field as documented by \newcite{wachs}. \newcite{wachs} also divulge the challenges in the field. One such challenge is correctly identifying and recognizing a gesture, from identifying the beginning and end of a gesutre, to correctly identifying what the gesture is.\\

In foundation of much of the work done in with gesture recognition in use with digital systems is based on the work of \newcite{rubine}, which not only worked with recognition of symbols using traditional inputs as well as touch inputs but also worked with teaching developed systems new gestures on the fly, something which the auther has yet to see in the consumer market.
\section{Method}
In this section I will expand upon the methods used to create obtain the results, as well as highlight collaborative as well as individual work conducted by Klaes Rasmussen and I.\\
\subsection{Data set}
The data set consists of \(80\) images split into four different sets of \(20\) images, \(40\) images of the "peace" gesture as well as \(40\) images of the "thumbs up" gesture. Out of each of these sets of \(40\) images each set is split into two sets of \(20\) images, each set containing either drawn or photographed images of the gesture. The reason for this split, is that I estimated that a trianing set of \(32\) images and a test set of \(8\) images would be enough to generalise the method and results, this has however not been the case and given time I would have increased the training set to at least \(100\) images in each split and then reevaluate the need.\\
Each image was found via simple image search, cropped, and resized into \(256x256\) pixels to ensure a consistent and comparable data set. Collecting and preparing the data set for analysis was a collaborative effort.\\
\subsection{Features}
Klaes and I decided to use Canny edges as features in the training data. The benefit of canny edge detection is that, not only are you given points but edges unlike other feature extractors such as Harris Corner detectors. Furthermore Canny edge detection has been shown by \newcite{badstudy} to be as good if not better than a number of other edge detection approaches.\\
\subsection{Classifiers}
Klaes and I wrote the code for the classification together, it was however utilised on two different data sets. Both Klaes and I trained our own models on our respective data sets and only used each others data set for testing how well our models generalised to another domain.\\
For my part I trained and tested my data set on images of human hands. Furthermore I performed a cross domain adaptation of my models onto the data set that Klaes used for his training and test.\\

I constructed three classification models: a support vector machine classifier, a logistic regression classification and a K-nearest neighbour classifier. The three classifiers work with significant differences to one another. The support vector machine attempts to find a hyperplane, in which the classes are linearly seperable (as the best kernel choice was a linear kernel), k-nearest neighbour classifies according to the closest cluster of data points, and logistic regression classification uses the training data to calculate whether a single instance is discriminately in one of two classes. For each classifier it is worth noting, that while I am working on a two class problem, they can all be used natively, or be extended to work for multi-class problems, which is necessary in case of putting such a classifier into production.\\
In order to ensure that our models weren't performing optimally due to bad choices of parameters for the models, we used a grid search and withhold-one cross validation to find the best model, using accuracy as the qualifying metric, as it is also used as the metric upon which I evaluate my models.\\
\section{Results}
Tables \ref{tab:TrainAcc} and \ref{tab:randomSeed} show the mean training accuracy over five folds, the standard deviation and the optimal parameters as found by the grid search using cross validation. Tables \ref{tab:seedDomainAcc} and \ref{tab:testDomainAcc} show the accuracy of the models on the test set as well as the accuracy of the models on the data set of drawn images. \\

\begin{table}[h]
  \centering
  \small
  \setlength\tabcolsep{2pt}
    \begin{tabular}{l|ccc}
    Classifier & Train Mean Acc & Std      & Parameters                     \\\hline
    SVM        & 62.50\%        & 16.245\% & 'kernel': 'linear', 'C': 0.001 \\
    LogReg     & 62.500\%       & 12.472\% & 'C': 0.01                      \\
    KNN        & 50.00\%        & 0.000\%  & 'n\_neighbors': 5
    \end{tabular}
  \caption{Best training parameters \& results, random seed set to 62.}
  \label{tab:TrainAcc}
\end{table}
\begin{table}[h]
  \centering
    \begin{tabular}{l|cc}
    Classifier & Test Acc & Acc on Drawings \\\hline
    SVM        & 62.5\%   & 87.5\%          \\
    LogReg     & 87.5\%   & 75.5\%          \\
    KNN        & 50\%     & 87.5\%
    \end{tabular}
  \caption{Accuracies on test set and different domain, random seed set to 62.}
  \label{tab:testDomainAcc}
\end{table}

As we see from the training and the test scores that there is are large differences in how well the models generalise onto unknown data. It is clear that there must be some discrepency with the data set given, that the logistic regression model has a mean accuracy on the cross validated data set that is \(25\%\) lower than accuracy on the test data. I had expected to for the accuracy to be roughly the same for the test data as for the training data, if not significantly lower. Furthermore it is interesting to note that the SVM performs signficiantly better on the test set from a different domain than it does on the test set within the same domain.\\
Despite these notes, the system does manage to classify well enough to warrant a further investigation into using canny edges, amongst other features to classify gestures in still images.

The most similar systems I have found are, such as \newcite{kipp} work with slightly different type of data or seek different measures of success, meaning that I can't relate my results to any other systems that I have found. As such I have chosen to use table \ref{tab:seedDomainAcc} as a baseline for my results, which all three models manage to beat.\\

\section{Discussion}
As mentioned there are a number of interesting things wrt. the results. Considering the outperformance of the models comparing the training scores and the test scores, I would argue that it is greatly influenced by the fact that the size of the data set is so relatively small, thus not giving the model enough data to effectively learn and generalise on the training data. I would argue that the same applies to the vastly better scores on the set of the different domain. Given that with not enough data, if just a few features are similar, then the image will be classed according to those few matches, rather than the dissimilarities.\\
The fact that the size of the data set was not large enough to generalise is portrayed through table \ref{tab:randomSeed} and table \ref{tab:TrainAcc}, given the discrepencies between the two. The only difference in achieving those two sets of results, was setting the random seed, and given that randomising has such a large impact on the scores, it would be a fair to assume that more data is needed to put forth a convincing argument for the system working as intended.\\

\begin{table}[h]
  \centering
  \small
  \setlength\tabcolsep{2pt}
    \begin{tabular}{l|ccc}
    Classifier & Train Mean Acc & Std      & Parameters                     \\\hline
    SVM        & 59.375\%       & 10.498\% & 'kernel': 'linear', 'C': 0.001 \\
    LogReg     & 62.500\%       & 12.472\% & 'C': 1.0                       \\
    KNN        & 50.00\%        & 0.00\%   & 'n\_neighbors': 5
    \end{tabular}
    \caption{Best training parameters \& results, random seed set to 42.}
  \label{tab:randomSeed}
\end{table}

\begin{table}[h]
  \centering
    \begin{tabular}{l|cc}
    Classifier & Test Acc & Acc on Drawings \\\hline
    SVM        & 50.0\%   & 50.0\%          \\
    LogReg     & 62.5\%   & 50.0\%          \\
    KNN        & 37.5\%   & 50.0\%
    \end{tabular}
  \caption{Accuracies on test set and different domain, random seed set to 42.}
  \label{tab:seedDomainAcc}
\end{table}
It is also interesting to note, for the reasonable results (namely the logistic regression) in table \ref{tab:testDomainAcc}, that the classifier performs reasonably well on the test set, without use of anything but the very basic canny edges. My results suggest that a simple implementation of vision recognition for specific gestures is not only possible but effective using only simple features. In addition they suggest that with simple features it is possible to perform cross domain adaptation on drawn representations of gestures. 
\section{Conclusion}
In this study we have found that not is it possible to recognise simple gestures with simple methods achieving a high accuracy, but also that the same systems can be used to recognise drawn representation of the same gestures achieving an equally high accuracy, with a relatively naive approach.\\
This work can be used as early stage results indicating an ability an an approach to improve gesture recognition and make simply software utilising the cameras that exist in most computing devices sold currently, to integrate more natural modes of communication along with the improvement and the increasing presence of speech technology.
\subsection{Further research}
The most obvious next step would be to increase the size of the training and test data sets to see if any convergence appears, the drawn data set does not immediately need to be expanded but it could have some interesting implications to see whether the results scale from 40 to 200 images. In extension of that it would be interesting to consider what other types of features could be interesting to look at, for example one might imagine that extracting skin colour and using those colour differences to detect edges, to be used as features could improve the results along with other features.
Another natural next step would be to perform an fMRI, showing images of human and drawn gestures to see if comparative neurological responses  to \newcite{yuasa} are found. Other interests could be to experiment to see whether or not drawings of gestures shown in concert with reading material increases recall in the human mind, not unlike the work done by \newcite{wing}.\\
To add to the likelihood of employment of such a system in industry, it would be interesting to use short movie sequences showing the gestures, emblems, in a varied set of lighting and distance, to see if not only does the system pick up on the gesture when movement is added, but also whether it picks up on the gesture in spite of depth and lighting changes.
\bibliographystyle{acl}
\bibliography{reference}
\end{document}

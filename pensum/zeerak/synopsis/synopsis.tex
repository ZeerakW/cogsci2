\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
% \usepackage{minted}
\usepackage{listingsutf8}
\usepackage{url}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{adjustbox} % Used to scale size of tables
\usepackage{acl2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{array}


\begin{document}
% \sloppy
\title{Cognitive Science 2 synopsis}
\author{Zeerak Waseem Butt (csp265)}
\date{\today}
\maketitle
\section{Abstract}
Given the increased use of digital media to communicate over time, it is necessary to create systems that can more accurately communicate as intended. In this project I will implement a machine learning system, that is trained on human hands gesturing 'thumbs up' and 'peace'. The system is tested on drawn images of the same gestures, thus indicating that further research into machine learning systems on drawn gestures can be used to preprogram human gestures in human computer interaction as well as computer mediated human interaction.\\

Note: The code is implemented with Klaes Rasmussen, as we both wanted to do image recognition on gestures and it allows us to test on each others training set. The suggested improvements, if there is further research, is more likely to be robust if both systems are worked upon.
\section{Introduction}
I am seeking to investigate cross domain adaptation of features in images portraying emblems onto images portraying the same drawn or animated emblems. Given the increase digital communication, as documented in \newcite{pew} and the different forms of interacting with a device, from voice to image recognition it is necessary to be able to map gestures, icons, and words to and from digital media, ensuring that our message is delivered with as little loss of non-verbal information as possible.\\
An act of face to face communication will often include gestures to indicate emotions, as \newcite{kipp} argue. Upon using digital media this mode of non-verbal communication is lost as well as the intonation of the speaker. In order to bridge this gap there digital communications, emoticons and smileys have been taken into use, as \newcite{yuasa} document. \newcite{wachs} divulge that integrating systems for multimodal communication using next generation IT systems will require not only effective and fast mapping from physical space and movement, but also the accurate translation between digital representations of gestures and physcally performed gestures. This is particularly important, considering the results achieved by \newcite{yuasa} showing that abstract representation of faces communicate emotion, thus it is prudent to consider whether representations of gestures elicit a similar reaction.\\
Due to the time constraings I will focus on mapping emblems performed by humans in still images onto digital representations of the same emblems in this study.
\section{Previous Work}
\newcite{ekman} argues that not only is there a set of emotions but also that in encounters between "organisms" conveying emotions have a critical role in prompting the feeling organism to react quickly to a given impression. The research \newcite{ekman} amongst others did allows further work such as the study by \newcite{kipp} investigating the link between gestures and emotions. Asides from \newcite{kipp} a lot of effort has gone into the field as documented by \newcite{wachs}. \newcite{wachs} also divulge the challenges in the field. One such challenge is correctly identifying and recognizing a gesture, from identifying the beginning and end of a gesutre, to correctly identifying what the gesture is.
\section{Method}
In this section I will expand upon the methods used to create obtain the results, as well as highlight collaborative as well as individual work conducted by Klaes Rasmussen and I.
\subsection{Data set}
The data set consists of \(80\) images split into four different sets of \(20\) images, \(40\) images of the "peace" gesture as well as \(40\) images of the "thumbs up" gesture. Out of each of these sets of \(40\) images each set is split into two sets of \(20\) images, each set containing either drawn or photographed images of the gesture. The reason for this split, is that I estimated that a trianing set of \(32\) images and a test set of \(8\) images would be enough to generalise the method and results, this has however not been the case and given time I would have increased the training set to at least \(100\) images in each split and then reevaluate the need.\\
Each image was found via simple image search, cropped, and resized into \(256x256\) pixels to ensure a consistent and comparable data set. Collecting and preparing the data set for analysis was a collaborative effort.\\
\subsection{Classifiers}
Klaes and I wrote the code for the classification together, it was however utilised on two different data sets. Both Klaes and I trained our own models on our respective data sets and only used each others data set for testing how well our models generalised to another domain.\\
For my part I trained and tested my data set on images of human hands. Furthermore I performed a cross domain adaptation of my models onto the data set that Klaes used for his training and test.\\

I constructed three classification models: a support vector machine classifier, a logistic regression classification and a K-nearest neighbour classifier. The three classifiers work with significant differences to one another. The support vector machine attempts to find a hyperplane, in which the classes are linearly seperable (as the best kernel choice was a linear kernel), k-nearest neighbour classifies according to the closest cluster of data points, and logistic regression ...
\section{Results}

\begin{table}[h]
  \centering
  \small
  \setlength\tabcolsep{2pt}
    \begin{tabular}{l|ccc}
    Classifier & Train Mean Acc & Std     & Parameters                     \\\hline
    SVM        & 0.6250         & 0.16245 & 'kernel': 'linear', 'C': 0.001 \\
    LogReg     & 0.62500        & 0.12472 & 'C': 0.01                      \\
    KNN        & 0.5000         & 0.0000  & 'n\_neighbors': 5
    \end{tabular}
  \caption{Best training parameters \& results}
  \label{fig:TrainAcc}
\end{table}

\begin{table}[h]
  \centering
    \begin{tabular}{l|cc}
    Classifier & Test Acc & Acc on Drawings \\\hline
    SVM        & 62.5\%   & 87.5\%               \\
    LogReg     & 87.5\%   & 75.5\%               \\
    KNN        & 50\%     & 87.5\%
    \end{tabular}
  \caption{Accuracies on test set and different domain}
  \label{fig:testDomainAcc}
\end{table}
\section{Discussion}
\section{Conclusion}
\subsection{Further research}
One natural next step would be to perform an fMRI, showing images of human and drawn gestures to see if comparative neurological responses are found.
\clearpage
\bibliographystyle{acl}
\bibliography{reference}
\nocite{*}
\end{document}
